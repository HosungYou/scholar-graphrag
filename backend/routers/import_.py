"""
Import API Router

Handles data import from ScholaRAG folders, PDFs, and CSVs.
"""

import logging
import os
from pathlib import Path
from typing import Optional, List
from uuid import UUID
from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Query
from pydantic import BaseModel
from datetime import datetime
from enum import Enum

from database import db
from graph.graph_store import GraphStore
from importers.scholarag_importer import ScholarAGImporter

logger = logging.getLogger(__name__)

router = APIRouter()


# ============================================================================
# Folder Browser API - íŒŒì¼ íƒìƒ‰ê¸° ê¸°ëŠ¥
# ============================================================================

class FolderItem(BaseModel):
    """í´ë”/íŒŒì¼ í•­ëª©"""
    name: str
    path: str
    is_directory: bool
    is_scholarag_project: bool = False  # config.yamlì´ ìˆëŠ” í´ë”ì¸ì§€
    has_subprojects: bool = False  # projects í•˜ìœ„ í´ë”ì— í”„ë¡œì íŠ¸ê°€ ìˆëŠ”ì§€


class BrowseResponse(BaseModel):
    """í´ë” ë¸Œë¼ìš°ì§• ì‘ë‹µ"""
    current_path: str
    parent_path: Optional[str]
    items: List[FolderItem]
    is_scholarag_project: bool = False
    suggested_projects: List[FolderItem] = []  # ìë™ ê°ì§€ëœ í”„ë¡œì íŠ¸ ëª©ë¡


class DiscoveredProject(BaseModel):
    """ë°œê²¬ëœ ScholaRAG í”„ë¡œì íŠ¸"""
    name: str
    path: str
    papers_count: int = 0
    has_config: bool = True


def _is_scholarag_project(folder: Path) -> bool:
    """í´ë”ê°€ ScholaRAG í”„ë¡œì íŠ¸ì¸ì§€ í™•ì¸"""
    config_path = folder / "config.yaml"
    return config_path.exists()


def _get_safe_home_paths() -> List[str]:
    """ì•ˆì „í•œ ì‹œì‘ ê²½ë¡œ ëª©ë¡ ë°˜í™˜"""
    home = Path.home()
    paths = [
        str(home),
        str(home / "Documents"),
        str(home / "Desktop"),
        str(home / "Downloads"),
    ]
    # macOSì˜ /Volumes ê²½ë¡œë„ í—ˆìš©
    volumes = Path("/Volumes")
    if volumes.exists():
        paths.append(str(volumes))
    return paths


def _is_path_allowed(path: str) -> bool:
    """ê²½ë¡œ ì ‘ê·¼ì´ í—ˆìš©ë˜ëŠ”ì§€ í™•ì¸ (ë³´ì•ˆ)"""
    try:
        resolved = Path(path).resolve()
        # í™ˆ ë””ë ‰í† ë¦¬ ë˜ëŠ” /Volumes í•˜ìœ„ë§Œ í—ˆìš©
        home = Path.home().resolve()
        allowed_roots = [home, Path("/Volumes").resolve()]
        return any(
            str(resolved).startswith(str(root)) or resolved == root
            for root in allowed_roots
        )
    except Exception:
        return False


@router.get("/browse", response_model=BrowseResponse)
async def browse_folder(
    path: Optional[str] = Query(None, description="íƒìƒ‰í•  í´ë” ê²½ë¡œ (ì—†ìœ¼ë©´ í™ˆ ë””ë ‰í† ë¦¬)"),
):
    """
    í´ë” ë‚´ìš©ì„ íƒìƒ‰í•©ë‹ˆë‹¤.

    ë³´ì•ˆ: ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬ ë° /Volumes í•˜ìœ„ë§Œ ì ‘ê·¼ ê°€ëŠ¥
    """
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    if not path:
        path = str(Path.home())

    folder = Path(path)

    # ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬
    if not folder.exists():
        raise HTTPException(status_code=404, detail=f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

    if not folder.is_dir():
        raise HTTPException(status_code=400, detail="íŒŒì¼ì´ ì•„ë‹Œ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    # ë³´ì•ˆ ê²€ì‚¬
    if not _is_path_allowed(path):
        raise HTTPException(status_code=403, detail="ì ‘ê·¼ì´ í—ˆìš©ë˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤")

    # ë¶€ëª¨ ê²½ë¡œ ê³„ì‚°
    parent_path = None
    if folder.parent != folder:  # ë£¨íŠ¸ê°€ ì•„ë‹ˆë©´
        parent = folder.parent
        if _is_path_allowed(str(parent)):
            parent_path = str(parent)

    # í´ë” ë‚´ìš© ì½ê¸°
    items: List[FolderItem] = []
    suggested_projects: List[FolderItem] = []

    try:
        for entry in sorted(folder.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower())):
            # ìˆ¨ê¹€ íŒŒì¼ ì œì™¸ (. ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼)
            if entry.name.startswith('.'):
                continue

            is_dir = entry.is_dir()
            is_project = False
            has_subprojects = False

            if is_dir:
                is_project = _is_scholarag_project(entry)
                # projects í•˜ìœ„ í´ë” í™•ì¸
                projects_subdir = entry / "projects"
                if projects_subdir.exists() and projects_subdir.is_dir():
                    try:
                        for sub in projects_subdir.iterdir():
                            if sub.is_dir() and _is_scholarag_project(sub):
                                has_subprojects = True
                                suggested_projects.append(FolderItem(
                                    name=sub.name,
                                    path=str(sub),
                                    is_directory=True,
                                    is_scholarag_project=True,
                                ))
                    except PermissionError:
                        pass

            items.append(FolderItem(
                name=entry.name,
                path=str(entry),
                is_directory=is_dir,
                is_scholarag_project=is_project,
                has_subprojects=has_subprojects,
            ))
    except PermissionError:
        raise HTTPException(status_code=403, detail="í´ë”ë¥¼ ì½ì„ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤")

    return BrowseResponse(
        current_path=str(folder),
        parent_path=parent_path,
        items=items,
        is_scholarag_project=_is_scholarag_project(folder),
        suggested_projects=suggested_projects,
    )


@router.get("/browse/quick-access")
async def get_quick_access_paths():
    """
    ë¹ ë¥¸ ì ‘ê·¼ ê²½ë¡œ ëª©ë¡ ë°˜í™˜ (í™ˆ, Documents, Volumes ë“±)
    """
    home = Path.home()
    paths = []

    # ê¸°ë³¸ ê²½ë¡œë“¤
    default_paths = [
        ("í™ˆ", home),
        ("Documents", home / "Documents"),
        ("Desktop", home / "Desktop"),
        ("Downloads", home / "Downloads"),
    ]

    for name, p in default_paths:
        if p.exists():
            paths.append({
                "name": name,
                "path": str(p),
                "icon": "folder"
            })

    # ì™¸ì¥ ë“œë¼ì´ë¸Œ (macOSì˜ /Volumes)
    volumes = Path("/Volumes")
    if volumes.exists():
        try:
            for vol in volumes.iterdir():
                if vol.is_dir() and not vol.name.startswith('.'):
                    # ë‚´ì¥ ë””ìŠ¤í¬ (Macintosh HD) ì œì™¸
                    if vol.name != "Macintosh HD":
                        paths.append({
                            "name": f"ğŸ“ {vol.name}",
                            "path": str(vol),
                            "icon": "hard-drive"
                        })
        except PermissionError:
            pass

    return {"paths": paths}


@router.post("/scholarag/discover")
async def discover_scholarag_projects(path: str = Query(..., description="íƒìƒ‰í•  ë£¨íŠ¸ ê²½ë¡œ")):
    """
    ì£¼ì–´ì§„ ê²½ë¡œì—ì„œ ScholaRAG í”„ë¡œì íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.

    ë£¨íŠ¸ í´ë”ë‚˜ projects í´ë”ë¥¼ ì…ë ¥í•˜ë©´ í•˜ìœ„ì˜ ëª¨ë“  í”„ë¡œì íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    """
    folder = Path(path)

    if not folder.exists():
        raise HTTPException(status_code=404, detail=f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

    if not _is_path_allowed(path):
        raise HTTPException(status_code=403, detail="ì ‘ê·¼ì´ í—ˆìš©ë˜ì§€ ì•Šì€ ê²½ë¡œì…ë‹ˆë‹¤")

    projects: List[DiscoveredProject] = []

    # í˜„ì¬ í´ë”ê°€ í”„ë¡œì íŠ¸ì¸ì§€ í™•ì¸
    if _is_scholarag_project(folder):
        papers_count = _count_papers(folder)
        projects.append(DiscoveredProject(
            name=folder.name,
            path=str(folder),
            papers_count=papers_count,
        ))

    # projects í•˜ìœ„ í´ë” íƒìƒ‰
    projects_dir = folder / "projects"
    if projects_dir.exists():
        for sub in projects_dir.iterdir():
            if sub.is_dir() and _is_scholarag_project(sub):
                papers_count = _count_papers(sub)
                projects.append(DiscoveredProject(
                    name=sub.name,
                    path=str(sub),
                    papers_count=papers_count,
                ))

    # ì§ì ‘ í•˜ìœ„ í´ë”ë„ íƒìƒ‰ (depth 1)
    if not projects and folder.is_dir():
        try:
            for sub in folder.iterdir():
                if sub.is_dir() and _is_scholarag_project(sub):
                    papers_count = _count_papers(sub)
                    projects.append(DiscoveredProject(
                        name=sub.name,
                        path=str(sub),
                        papers_count=papers_count,
                    ))
        except PermissionError:
            pass

    return {
        "root_path": path,
        "projects": projects,
        "count": len(projects),
    }


def _count_papers(folder: Path) -> int:
    """í´ë”ì—ì„œ ë…¼ë¬¸ ìˆ˜ ì¹´ìš´íŠ¸"""
    csv_paths = [
        folder / "data" / "02_screening" / "relevant_papers.csv",
        folder / "data" / "02_screening" / "all_screened_papers.csv",
    ]

    for csv_path in csv_paths:
        if csv_path.exists():
            try:
                with open(csv_path, "r", encoding="utf-8") as f:
                    return max(0, sum(1 for _ in f) - 1)
            except Exception:
                pass
    return 0


# ============================================================================
# Import API
# ============================================================================


# Enums
class ImportStatus(str, Enum):
    PENDING = "pending"
    VALIDATING = "validating"
    EXTRACTING = "extracting"
    PROCESSING = "processing"
    BUILDING_GRAPH = "building_graph"
    COMPLETED = "completed"
    FAILED = "failed"


# Pydantic Models
class ScholaRAGImportRequest(BaseModel):
    folder_path: str
    project_name: Optional[str] = None
    extract_entities: bool = True  # Use LLM to extract Concept, Method, Finding


class ImportJobResponse(BaseModel):
    job_id: str
    status: ImportStatus
    progress: float  # 0.0 to 1.0
    message: str
    project_id: Optional[UUID] = None
    stats: Optional[dict] = None
    created_at: datetime
    updated_at: datetime


class ImportValidationResponse(BaseModel):
    valid: bool
    folder_path: str
    config_found: bool
    scholarag_metadata_found: bool
    papers_csv_found: bool
    papers_count: int
    pdfs_count: int
    chroma_db_found: bool
    errors: list = []
    warnings: list = []


# In-memory storage
_import_jobs: dict = {}


@router.post("/scholarag/validate", response_model=ImportValidationResponse)
async def validate_scholarag_folder(request: ScholaRAGImportRequest):
    """
    Validate a ScholaRAG project folder before import.

    Checks:
    - config.yaml exists
    - .scholarag metadata exists
    - data/02_screening/relevant_papers.csv exists
    - data/03_pdfs/ contains PDFs
    - data/04_rag/chroma_db/ exists (optional)
    """
    from pathlib import Path

    folder = Path(request.folder_path)

    validation = ImportValidationResponse(
        valid=True,
        folder_path=request.folder_path,
        config_found=False,
        scholarag_metadata_found=False,
        papers_csv_found=False,
        papers_count=0,
        pdfs_count=0,
        chroma_db_found=False,
        errors=[],
        warnings=[],
    )

    # Check folder exists
    if not folder.exists():
        validation.valid = False
        validation.errors.append(f"Folder not found: {request.folder_path}")
        return validation

    # Check config.yaml
    config_path = folder / "config.yaml"
    validation.config_found = config_path.exists()
    if not validation.config_found:
        validation.errors.append("config.yaml not found")
        validation.valid = False

    # Check .scholarag
    metadata_path = folder / ".scholarag"
    validation.scholarag_metadata_found = metadata_path.exists()
    if not validation.scholarag_metadata_found:
        validation.warnings.append(".scholarag metadata not found (optional)")

    # Check papers CSV - try multiple possible locations
    csv_candidates = [
        ("data/02_screening/relevant_papers.csv", "relevant_papers.csv"),
        ("data/02_screening/all_screened_papers.csv", "all_screened_papers.csv"),
        ("data/02_screening/screened_papers.csv", "screened_papers.csv"),
        ("data/01_identification/papers.csv", "papers.csv in identification"),
        ("data/papers.csv", "papers.csv in data root"),
    ]

    found_csv_path = None
    for csv_rel_path, csv_name in csv_candidates:
        csv_path = folder / csv_rel_path
        if csv_path.exists():
            found_csv_path = csv_path
            validation.papers_csv_found = True
            if csv_name != "relevant_papers.csv":
                validation.warnings.append(f"Using {csv_name} instead of relevant_papers.csv")
            break

    if found_csv_path:
        # Count rows (excluding header)
        try:
            with open(found_csv_path, "r", encoding="utf-8") as f:
                validation.papers_count = max(0, sum(1 for _ in f) - 1)
        except Exception as e:
            validation.warnings.append(f"Could not count papers: {e}")

    if not validation.papers_csv_found:
        validation.errors.append(
            "ì´ í”„ë¡œì íŠ¸ì—ëŠ” ì•„ì§ ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. "
            "ScholaRAGì—ì„œ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. "
            "(í•„ìš”: data/02_screening/relevant_papers.csv)"
        )
        validation.valid = False

    # Check PDFs - try multiple possible locations
    pdf_dirs = [
        folder / "data" / "03_pdfs",
        folder / "data" / "pdfs",
        folder / "data" / "03_full_text",
        folder / "pdfs",
    ]

    for pdfs_dir in pdf_dirs:
        if pdfs_dir.exists():
            validation.pdfs_count = len(list(pdfs_dir.rglob("*.pdf")))
            break

    if validation.pdfs_count == 0:
        validation.warnings.append("PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)")

    # Check ChromaDB
    chroma_dir = folder / "data" / "04_rag" / "chroma_db"
    validation.chroma_db_found = chroma_dir.exists() and any(chroma_dir.iterdir()) if chroma_dir.exists() else False

    return validation


@router.post("/scholarag", response_model=ImportJobResponse)
async def import_scholarag_folder(
    request: ScholaRAGImportRequest,
    background_tasks: BackgroundTasks,
):
    """
    Start importing a ScholaRAG project folder.

    Process:
    1. Validate folder structure
    2. Parse config.yaml â†’ Create Project
    3. Parse relevant_papers.csv â†’ Create Paper entities
    4. Extract Authors from papers
    5. (If extract_entities=True) Use LLM to extract Concept, Method, Finding
    6. Build relationships (AUTHORED_BY, DISCUSSES_CONCEPT, etc.)
    7. Store in PostgreSQL + pgvector
    """
    from uuid import uuid4

    # Create import job
    job_id = str(uuid4())
    now = datetime.now()

    job = ImportJobResponse(
        job_id=job_id,
        status=ImportStatus.PENDING,
        progress=0.0,
        message="Import job created",
        project_id=None,
        stats=None,
        created_at=now,
        updated_at=now,
    )

    _import_jobs[job_id] = job.model_dump()

    # Start background import task
    background_tasks.add_task(
        _run_scholarag_import,
        job_id=job_id,
        folder_path=request.folder_path,
        project_name=request.project_name,
        extract_entities=request.extract_entities,
    )

    return job


async def _run_scholarag_import(
    job_id: str,
    folder_path: str,
    project_name: Optional[str],
    extract_entities: bool,
):
    """Background task to run ScholaRAG import."""
    logger.info(f"[Import {job_id}] Starting import from: {folder_path}")

    def progress_callback(progress):
        """Update job status from importer progress."""
        status_map = {
            "validating": ImportStatus.VALIDATING,
            "extracting": ImportStatus.EXTRACTING,
            "processing": ImportStatus.PROCESSING,
            "building_graph": ImportStatus.BUILDING_GRAPH,
            "completed": ImportStatus.COMPLETED,
            "failed": ImportStatus.FAILED,
        }
        _import_jobs[job_id]["status"] = status_map.get(progress.status, ImportStatus.PROCESSING)
        _import_jobs[job_id]["progress"] = progress.progress
        _import_jobs[job_id]["message"] = progress.message
        _import_jobs[job_id]["updated_at"] = datetime.now()
        logger.info(f"[Import {job_id}] {progress.status}: {progress.progress:.1%} - {progress.message}")

    try:
        # Create importer with database connection and GraphStore
        graph_store = GraphStore(db=db)
        importer = ScholarAGImporter(
            db_connection=db,
            graph_store=graph_store,
            progress_callback=progress_callback,
        )

        # Run the import
        result = await importer.import_folder(
            folder_path=folder_path,
            project_name=project_name,
            extract_entities=extract_entities,
        )

        if result["success"]:
            # Update job with results
            _import_jobs[job_id]["status"] = ImportStatus.COMPLETED
            _import_jobs[job_id]["progress"] = 1.0
            _import_jobs[job_id]["message"] = "Import completed successfully!"
            _import_jobs[job_id]["project_id"] = result.get("project_id")
            _import_jobs[job_id]["stats"] = result.get("stats", {})
            _import_jobs[job_id]["updated_at"] = datetime.now()

            logger.info(f"[Import {job_id}] Completed successfully: {result.get('stats', {})}")
        else:
            # Import failed
            _import_jobs[job_id]["status"] = ImportStatus.FAILED
            _import_jobs[job_id]["message"] = f"Import failed: {result.get('error', 'Unknown error')}"
            _import_jobs[job_id]["updated_at"] = datetime.now()
            _import_jobs[job_id]["stats"] = {
                "papers_imported": 0,
                "authors_extracted": 0,
                "concepts_extracted": 0,
                "relationships_created": 0,
            }

            logger.error(f"[Import {job_id}] Failed: {result.get('error')}")

    except Exception as e:
        logger.exception(f"[Import {job_id}] Exception during import: {e}")
        _import_jobs[job_id]["status"] = ImportStatus.FAILED
        _import_jobs[job_id]["message"] = f"Import failed: {str(e)}"
        _import_jobs[job_id]["updated_at"] = datetime.now()
        _import_jobs[job_id]["stats"] = {
            "papers_imported": 0,
            "authors_extracted": 0,
            "concepts_extracted": 0,
            "relationships_created": 0,
        }


@router.get("/status/{job_id}", response_model=ImportJobResponse)
async def get_import_status(job_id: str):
    """Get the status of an import job."""
    job = _import_jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Import job not found")
    return ImportJobResponse(**job)


@router.get("/jobs", response_model=List[ImportJobResponse])
async def get_all_import_jobs(
    status: Optional[ImportStatus] = Query(None, description="í•„í„°í•  ìƒíƒœ"),
    limit: int = Query(20, ge=1, le=100, description="ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜"),
):
    """
    ëª¨ë“  import job ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì€ ë§¨ ìœ„ì—, ì™„ë£Œ/ì‹¤íŒ¨ëœ ì‘ì—…ì€ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ë©ë‹ˆë‹¤.
    """
    jobs = list(_import_jobs.values())

    # Filter by status if provided
    if status:
        jobs = [j for j in jobs if j["status"] == status]

    # Sort: in-progress first, then by updated_at descending
    def sort_key(job):
        is_in_progress = job["status"] not in [ImportStatus.COMPLETED, ImportStatus.FAILED]
        return (-int(is_in_progress), -job["updated_at"].timestamp())

    jobs.sort(key=sort_key)

    return [ImportJobResponse(**j) for j in jobs[:limit]]


class PDFImportResponse(BaseModel):
    """PDF import response."""
    job_id: str
    status: str
    filename: str
    message: str
    paper_id: Optional[str] = None
    stats: Optional[dict] = None


@router.post("/pdf", response_model=PDFImportResponse)
async def import_pdf(
    project_id: UUID,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
):
    """
    Import a single PDF file into a project.

    Extracts text, metadata, and creates Paper/Author entities.
    """
    from uuid import uuid4
    from importers.pdf_importer import import_pdf_from_upload

    if not file.filename or not file.filename.lower().endswith(".pdf"):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

    job_id = str(uuid4())
    now = datetime.now()

    # Create import job
    job = {
        "job_id": job_id,
        "status": ImportStatus.PROCESSING,
        "progress": 0.0,
        "message": f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file.filename}",
        "project_id": project_id,
        "stats": None,
        "created_at": now,
        "updated_at": now,
    }
    _import_jobs[job_id] = job

    # Read file content
    try:
        file_content = await file.read()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    # Run import in background
    async def run_pdf_import():
        def progress_callback(progress):
            _import_jobs[job_id]["status"] = ImportStatus.PROCESSING if progress.status == "processing" else (
                ImportStatus.COMPLETED if progress.status == "completed" else ImportStatus.FAILED
            )
            _import_jobs[job_id]["progress"] = progress.progress
            _import_jobs[job_id]["message"] = progress.message
            _import_jobs[job_id]["updated_at"] = datetime.now()

        try:
            graph_store = GraphStore(db=db)
            result = await import_pdf_from_upload(
                file_content=file_content,
                filename=file.filename,
                project_id=str(project_id),
                db_connection=db,
                graph_store=graph_store,
                progress_callback=progress_callback,
            )

            if result["success"]:
                _import_jobs[job_id]["status"] = ImportStatus.COMPLETED
                _import_jobs[job_id]["progress"] = 1.0
                _import_jobs[job_id]["message"] = f"Import ì™„ë£Œ: {result.get('title', file.filename)}"
                _import_jobs[job_id]["stats"] = result.get("stats", {})
            else:
                _import_jobs[job_id]["status"] = ImportStatus.FAILED
                _import_jobs[job_id]["message"] = f"Import ì‹¤íŒ¨: {result.get('error', 'Unknown error')}"

            _import_jobs[job_id]["updated_at"] = datetime.now()

        except Exception as e:
            logger.exception(f"PDF import failed: {e}")
            _import_jobs[job_id]["status"] = ImportStatus.FAILED
            _import_jobs[job_id]["message"] = f"Import ì‹¤íŒ¨: {str(e)}"
            _import_jobs[job_id]["updated_at"] = datetime.now()

    background_tasks.add_task(run_pdf_import)

    return PDFImportResponse(
        job_id=job_id,
        status="processing",
        filename=file.filename,
        message=f"PDF import ì‹œì‘: {file.filename}",
    )


@router.post("/csv")
async def import_csv(
    project_id: UUID,
    file: UploadFile = File(...),
):
    """Import papers from a CSV file."""
    from uuid import uuid4

    if not file.filename or not file.filename.lower().endswith(".csv"):
        raise HTTPException(status_code=400, detail="File must be a CSV")

    job_id = str(uuid4())

    # TODO: Implement CSV import
    return {
        "job_id": job_id,
        "status": "pending",
        "filename": file.filename,
        "message": "CSV import not yet implemented",
    }
